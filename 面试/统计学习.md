## XGBoost等

1. xgboost**缺失值**处理方法
   1. 分裂的时候
   2. 指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。
   3. 忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销
   4. XGBoost 在构建树的节点过程中**只考虑非缺失值的数据遍历**，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，**分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。**
   5. 在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而特征值缺失的样本无需遍历只需直接分配到左右节点，故算法所需遍历的样本量减少，稀疏感知算法比 basic 算法速度块了超过 50 倍。
   6. 实际上就是对缺失值的处理，现在没有缺失的值上分裂，然后把缺失值分别带入左节点算一下分裂后的增益，再带入右节点算一下分裂后的增益然后去其中大的一个作为最终的分裂方案。如果训练中没有数据缺失，预测时出现了数据缺失，则默认被分类到右节点，原作者在这里是把缺失值描述为稀疏矩阵引起了歧义，实际上很我们常规意义上理解的那种onehot之后一大堆0的情况不一样，lightgbm的efb特征捆绑才是真正对这种0很多的特征进行合并从而实现稀疏特征的优化。
2. **为什么xgboost要二阶展开**
   1. 二阶泰勒展开的优势是相对于一阶而言的，和牛顿方法相对于梯度下降类似，都是为了更准确的找到最优解，这才是重点。
3. 123

### 参考

[一篇文章搞定GBDT、Xgboost和LightGBM的面试](https://zhuanlan.zhihu.com/p/148050748)



## 优化方法

1. 除了梯度下降，你还了解哪些优化方法
2. 