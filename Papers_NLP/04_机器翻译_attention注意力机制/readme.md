# 注意力机制的核心——软对齐模型 理解

> 注意力机制的核心其实是一个对齐模型，但是讲解视频讲得不好，论文描述也不好懂，所以造成了一些理解的困难，这里来重点梳理一下。

!['dyngq_images'](images/dyngq_2019-10-28-17-04-37.png)

在生成下一个目标词之前，我们需要一个上下文向量contact_vector和一个已经生成的序列，
