{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# jupyter notebook test"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"dyngq\n"}],"source":["print('dyngq')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"VSCode NB\n"}],"source":["print('VSCode NB')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"1.1.0\n"}],"source":["import torch as tf\n","print(tf.__version__)"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Word2Vector\n\n!['dyngq_images'](images/dyngq_2019-10-09-16-47-42.png)\n\n## 入门NLP课程\n\n!['dyngq_images'](images/dyngq_2019-10-09-16-50-10.png)\n\n### 语言模型\n\n!['dyngq_images'](images/dyngq_2019-10-09-16-51-01.png)\n!['dyngq_images'](images/dyngq_2019-10-09-17-02-29.png)\n\n有些词组合在一起的概率是很小很小的，所以会比较稀疏，有很多是很小没有太大意义的,并且造成参数空间太大。\n!['dyngq_images'](images/dyngq_2019-10-09-17-03-55.png)\n\n#### 马尔科夫假设\n\n本状态至于前边的状态有关\n\n基于马尔科夫假设，提出：\n\n下一个词的出现仅依赖于它前面的一个词或几个词\n\n!['dyngq_images'](images/dyngq_2019-10-09-17-14-53.png)\n\n于是提出n-gram模型：\n!['dyngq_images'](images/dyngq_2019-10-09-20-19-23.png)\nn取1、2、3，称为unigram bigram trigram（一元二元三元语法）\nn同样可以取的更大\n\n[(四)N-gram语言模型与马尔科夫假设](https://blog.csdn.net/hao5335156/article/details/82730983)\n!['dyngq_images'](images/dyngq_2019-10-09-20-06-15.png)\n!['dyngq_images'](images/dyngq_2019-10-09-21-03-42.png)\n\n### 词向量\n\n#### One-Hot\n\n!['dyngq_images'](images/dyngq_2019-10-09-21-11-04.png)\n实际工程中很少使用One-Hot向量，无法处理新出现的词典里现在没有的词汇\n\n#### 分布式表示 Distributed Word Representation\n\nEmbedding || CBOW，Skip-gram（word2vec）\n\nEmbedding层一般可以选择：\n\n1. 加载预训练的词嵌入（比如常用Glove预训练的词嵌入）\n2. 利用Word2Vector通过自己的语料库自己训练词嵌入\n\n[参考链接：word2vec和word embedding有什么区别](https://www.zhihu.com/question/53354714)\n!['dyngq_images'](images/dyngq_2019-10-09-21-34-53.png)\n!['dyngq_images'](images/dyngq_2019-10-09-21-46-53.png)\n!['dyngq_images'](images/dyngq_2019-10-09-21-48-10.png)\n训练词向量\n!['dyngq_images'](images/dyngq_2019-10-09-21-49-38.png)\n\n## 精读论文\n\n!['dyngq_images'](images/dyngq_2019-10-10-11-40-46.png)\n不熟悉的知识点有NNLM/RNNLM、LSA、LDA\n\n> word2vec的本质就是一个语言模型\n\n!['dyngq_images'](images/dyngq_2019-10-10-17-26-59.png)\n\n**关于负采样**\n[（三）通俗易懂理解——Skip-gram的负采样](https://zhuanlan.zhihu.com/p/39684349)\n[word_embedding的负采样算法,Negative Sampling 模型](http://www.imooc.com/article/41635)\n!['dyngq_images'](images/dyngq_2019-10-10-19-45-11.png)\n[百度百科：负采样](https://baike.baidu.com/item/负采样/22884020?fr=aladdin)\n\n论文结构\n!['dyngq_images'](images/dyngq_2019-10-10-20-50-11.png)\n\nNNLM神经网路语言模型（Nerual Network Language Model）\n[神经网路语言模型(NNLM)的理解](https://blog.csdn.net/lilong117194/article/details/82018008)\n\n最大似然\n!['dyngq_images'](images/dyngq_2019-10-10-21-55-29.png)\n!['dyngq_images'](images/dyngq_2019-10-10-21-57-47.png)\n\n### 模型结构\n\n!['dyngq_images'](images/dyngq_2019-10-11-11-26-55.png)\n!['dyngq_images'](images/dyngq_2019-10-11-20-04-05.png)\n!['dyngq_images'](images/dyngq_2019-10-11-20-54-48.png)\n\n#### 具体过程\n\n1. 假设滑动窗口大小为 4。既：给定前面三个词 预测 第四个词会是谁出现的概率最大\n2. 输入为前三个词的one-hot，输出为第四个此的one-hot（第四个词已知，所以是监督学习，学习一个矩阵C）\n3. 初始化C矩阵。\n4. 输入为One-Hot，每个词只有一个位置为1，其他为0，所以经过与C矩阵相乘，得到每个词的一行自定义维（300维等）的属于自己的稠密向量。（投影层）\n5. 300维的稠密向量在隐藏层全连接（线：3 * 100 跟线），也就是说hidden layer每个神经元都有3条线相连，使用非线性的tan函数结合H和B进行激活输出。（隐藏层）\n6. 输出层采用全连接，有 100 * 10W(语料库词总数)条线。使用softmax结合U和D激活输出最后的概率，既第四个词为某个词的概率值（10W个全部的词语的所有概率）。（输出层）\n7. 反向传播更新 **矩阵C（最重要）**、H、B、U、D。\n\n* BP(back propagation)神经网络，反向传播\n* SGD（Stochastic gradient descent） 随机梯度下降\n\n!['dyngq_images'](images/dyngq_2019-10-11-21-46-44.png)\n\n参考资料：\n\n[神经网路语言模型(NNLM)的理解](https://blog.csdn.net/lilong117194/article/details/82018008)\n\n[NNLM(神经网络语言模型)](https://blog.csdn.net/maqunfi/article/details/84455434)\n\n[神经网络语言模型（NNLM）](https://www.jianshu.com/p/c28517cdfb3d)\n\n[神经网络语言建模系列之一：基础模型](https://www.jianshu.com/p/a02ea64d6459)\n\n计算复杂度：\n\n输入词语个数N、稠密词向量维度D、隐层节点数H、输出词表大小V\n!['dyngq_images'](images/dyngq_2019-10-12-10-52-28.png)\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}