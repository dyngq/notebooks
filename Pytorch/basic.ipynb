{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch 基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 基本运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[9.4592e-39, 8.4490e-39, 9.6428e-39],\n        [1.1112e-38, 9.5511e-39, 1.0102e-38],\n        [1.0286e-38, 1.0194e-38, 9.6429e-39],\n        [9.2755e-39, 9.1837e-39, 9.3674e-39],\n        [1.0745e-38, 1.0653e-38, 9.5510e-39]])\ntensor([[0.2957, 0.4942, 0.5437],\n        [0.2666, 0.6833, 0.9522],\n        [0.4790, 0.9809, 0.5076],\n        [0.1269, 0.9866, 0.8390],\n        [0.4247, 0.5785, 0.4810]])\ntensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([5.5000, 3.0000])\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\ntensor([[ 0.7131,  1.3377, -0.3636],\n        [ 0.9037, -0.1996,  0.4074],\n        [ 0.6950,  0.6657, -0.5387],\n        [ 0.9411, -0.4678, -0.1665],\n        [ 1.9600, -0.0802,  1.4722]])\ntorch.Size([5, 3])\ntorch.Size([5, 3])\n"
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "\n",
    "# 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)  \n",
    "print(x)\n",
    "\n",
    "# 指定新的数据类型 (改变数据类型，size相同，数据变为随机值)\n",
    "x = torch.randn_like(x, dtype=torch.float) \n",
    "print(x) \n",
    "\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 0.7131,  1.3377, -0.3636],\n        [ 0.9037, -0.1996,  0.4074],\n        [ 0.6950,  0.6657, -0.5387],\n        [ 0.9411, -0.4678, -0.1665],\n        [ 1.9600, -0.0802,  1.4722]]) tensor([[0.3286, 0.1477, 0.8456],\n        [0.9581, 0.1946, 0.3133],\n        [0.6281, 0.7076, 0.8745],\n        [0.9048, 0.1309, 0.9229],\n        [0.3110, 0.1187, 0.3409]])\ntensor([[ 1.0417,  1.4854,  0.4820],\n        [ 1.8618, -0.0049,  0.7207],\n        [ 1.3231,  1.3733,  0.3358],\n        [ 1.8458, -0.3370,  0.7564],\n        [ 2.2710,  0.0386,  1.8131]])\ntensor([[ 1.0417,  1.4854,  0.4820],\n        [ 1.8618, -0.0049,  0.7207],\n        [ 1.3231,  1.3733,  0.3358],\n        [ 1.8458, -0.3370,  0.7564],\n        [ 2.2710,  0.0386,  1.8131]])\ntensor([[ 1.0417,  1.4854,  0.4820],\n        [ 1.8618, -0.0049,  0.7207],\n        [ 1.3231,  1.3733,  0.3358],\n        [ 1.8458, -0.3370,  0.7564],\n        [ 2.2710,  0.0386,  1.8131]])\ntensor([[ 1.0417,  1.4854,  0.4820],\n        [ 1.8618, -0.0049,  0.7207],\n        [ 1.3231,  1.3733,  0.3358],\n        [ 1.8458, -0.3370,  0.7564],\n        [ 2.2710,  0.0386,  1.8131]])\ntensor([[ 1.0417,  1.8618,  1.3231,  1.8458,  2.2710],\n        [ 1.4854, -0.0049,  1.3733, -0.3370,  0.0386],\n        [ 0.4820,  0.7207,  0.3358,  0.7564,  1.8131]])\n"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x,y)\n",
    "\n",
    "# 三种加法操作\n",
    "\n",
    "print(x + y)\n",
    "\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# 指定输出\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "# 加法形式三、inplace (注意 后缀_ )\n",
    "y.add_(x)\n",
    "print(y)\n",
    "print(y.t_()) # 转置\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "函数 | 功能\n",
    ":-: | :-:\n",
    "Tensor(*sizes) | 基础构造函数\n",
    "tensor(data,) | 类似np.array的构造函数\n",
    "ones(*sizes) | 全1Tensor\n",
    "zeros(*sizes) | 全0Tensor\n",
    "eye(*sizes) | 对角线为1，其他为0\n",
    "arange(s,e,step) | 从s到e，步长为step\n",
    "linspace(s,e,steps) | 从s到e，均匀切分成steps份\n",
    "rand/randn(*sizes) | 均匀/标准分布\n",
    "normal(mean,std)/uniform(from,to) | 正态分布/均匀分布\n",
    "randperm(m) | 随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 索引\n",
    "\n",
    "我们还可以使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：**索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([50.7131, 51.3377, 49.6364])\ntensor([[50.7131, 51.3377, 49.6364],\n        [ 0.9037, -0.1996,  0.4074],\n        [ 0.6950,  0.6657, -0.5387],\n        [ 0.9411, -0.4678, -0.1665],\n        [ 1.9600, -0.0802,  1.4722]])\n"
    }
   ],
   "source": [
    "y = x[0, :]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x) # 源tensor也被改了 这一点特别重要"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "函数 | 功能\n",
    ":-: | :-:\n",
    "index_select(input, dim, index) | 在指定维度dim上选取，比如选取某些行、某些列\n",
    "masked_select(input, mask) | 例子如上，a\\[a>0] ，使用ByteTensor进行选取\n",
    "nonzero(input) | 非0元素的下标\n",
    "gather(input, dim, index) | 根据index，在dim维度上选取数据，输出的size与index一样"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 改变形状\n",
    "\n",
    "用view()来改变Tensor的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(), y.size(), z.size())\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "注意view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)\n",
    "\n",
    "### 感觉有一种视图的感觉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[51.7131, 52.3377, 50.6364],\n        [ 1.9037,  0.8004,  1.4074],\n        [ 1.6950,  1.6657,  0.4613],\n        [ 1.9411,  0.5322,  0.8335],\n        [ 2.9600,  0.9198,  2.4722]])\ntensor([51.7131, 52.3377, 50.6364,  1.9037,  0.8004,  1.4074,  1.6950,  1.6657,\n         0.4613,  1.9411,  0.5322,  0.8335,  2.9600,  0.9198,  2.4722])\n"
    }
   ],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y) # 也加了1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[50.7131, 51.3377, 49.6364],\n        [ 0.9037, -0.1996,  0.4074],\n        [ 0.6950,  0.6657, -0.5387],\n        [ 0.9411, -0.4678, -0.1665],\n        [ 1.9600, -0.0802,  1.4722]])\ntensor([51.7131, 52.3377, 50.6364,  1.9037,  0.8004,  1.4074,  1.6950,  1.6657,\n         0.4613,  1.9411,  0.5322,  0.8335,  2.9600,  0.9198,  2.4722])\n"
    }
   ],
   "source": [
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([-1.2833])\n-1.2833356857299805\n"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 线性代数\n",
    "\n",
    "另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示："
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "函数 | 功能\n",
    ":-: | :-:\n",
    "trace | 对角线元素之和(矩阵的迹)\n",
    "diag | 对角线元素\n",
    "triu/tril | 矩阵的上三角/下三角，可指定偏移量\n",
    "mm/bmm | 矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm.. | 矩阵运算\n",
    "t | 转置\n",
    "dot/cross | 内积/外积\n",
    "inverse | 求逆矩阵\n",
    "svd | 奇异值分解\n",
    "\n",
    "PyTorch中的Tensor支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考[官方文档](https://pytorch.org/docs/stable/tensors.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 广播机制\n",
    "\n",
    "前面我们看到如何对两个形状相同的Tensor做按元素运算。当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1, 2]]) tensor([1, 2])\ntensor([[1],\n        [2],\n        [3]])\ntensor([[2, 3],\n        [3, 4],\n        [4, 5]])\n"
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x, torch.arange(1, 3))\n",
    "\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-84-bc9f9d080848>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-84-bc9f9d080848>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    前面说了，索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用Python自带的id函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "## 运算的内存开销\n",
    "\n",
    "前面说了，索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用Python自带的 **id函数** ：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False 2005744637416 2005747238448\n"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before, id(y), id_before) # False 2005744637416 2005747238448"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果想指定结果到原来的y的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把x + y的结果通过\\[:]写进y对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True 2005745179600 2005745179600\n"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before, id(y), id_before) # True 2005747240608 2005747240608"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果，例如torch.add(x, y, out=y)和y += x(y.add_(x))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\n"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> 注：虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensor和NumPy相互转换"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们很容易用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： \n",
    "\n",
    "**这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！**\n",
    "\n",
    "还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor(), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tensor转NumPy\n",
    "\n",
    "使用numpy()将Tensor转换成NumPy数组:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\ntensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\ntensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NumPy数组转Tensor\n",
    "使用from_numpy()将NumPy数组转换成Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "所有在CPU上的Tensor（除了CharTensor）都支持与NumPy数组相互转换。\n",
    "\n",
    "此外上面提到还有一个常用的方法就是直接用torch.tensor()将NumPy数组转换成Tensor，需要注意的是该方法总是会进行数据拷贝，返回的Tensor和原来的数据**不再共享内存**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2. 2. 2. 2. 2.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "c = torch.tensor(a)\n",
    "a += 1\n",
    "print(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensor on GPU\n",
    "\n",
    "用方法to()可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([2, 3], device='cuda:0')\ntensor([2., 3.], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1\nGeForce GTX 1650\n0\n"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}