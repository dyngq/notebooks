{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "        self.predict1 = nn.Linear(n_hidden*2, n_output)\n",
    "        self.predict2 = nn.Linear(n_hidden*2, n_output)\n",
    "\n",
    "    def forward(self, input1, input2): # 多输入！！！\n",
    "        input1 = input1.view(30,3)\n",
    "        out01 = self.hidden1(input1)\n",
    "        out02 = torch.relu(out01)\n",
    "        out03 = self.hidden2(out02)\n",
    "        out04 = torch.sigmoid(out03)\n",
    "\n",
    "        input2 = input2.view(30,3)\n",
    "        out11 = self.hidden1(input2)\n",
    "        out12 = torch.relu(out11)\n",
    "        out13 = self.hidden2(out12)\n",
    "        out14 = torch.sigmoid(out13)\n",
    "\n",
    "        out = torch.cat((out04, out14), dim=1) # 模型层拼合！！！当然你的模型中可能不需要~\n",
    " \n",
    "        out = self.predict1(out)\n",
    "        return out\n",
    "#         out1 = self.predict1(out)\n",
    "#         out2 = self.predict2(out)\n",
    "\n",
    "#         return out1, out2 # 多输出！！！\n",
    "\n",
    "\n",
    "net = Net(3, 4, 1)\n",
    "\n",
    "\n",
    "x1 = torch.unsqueeze(torch.linspace(-1, 1, 90), dim=1) # 请不要关心这里，随便弄一个数据，为了说明问题而已\n",
    "y1 = x1.pow(3)+0.1*torch.randn(x1.size())\n",
    "\n",
    "x2 = torch.unsqueeze(torch.linspace(-1, 1, 90), dim=1)\n",
    "y2 = x2.pow(3)+0.1*torch.randn(x2.size())\n",
    "\n",
    "# x1, y1 = (Variable(x1), Variable(y1))\n",
    "# x2, y2 = (Variable(x2), Variable(y2))\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = y1[:30]\n",
    "# y1.size()\n",
    "y2 = y2[:30]\n",
    "# y2.size()\n",
    "# x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y2.type()\n",
    "\n",
    "# # print(x1.grad)\n",
    "# x1.requires_grad = True\n",
    "# # x1.requires_grad\n",
    "# # loss1 = (x1**2+1).mean()\n",
    "# # loss1.zero_grad\n",
    "# # print(loss1)\n",
    "# loss1 = (x1**2+1).mean()\n",
    "# loss1.backward()\n",
    "# # loss1.zero_grad()\n",
    "# print(x1.grad)\n",
    "# x1.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.0978\n",
      "-->name: hidden1.weight -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([[ 0.0055,  0.0053,  0.0051],\n",
      "        [ 0.0009,  0.0009,  0.0009],\n",
      "        [-0.0012, -0.0012, -0.0013],\n",
      "        [ 0.0034,  0.0033,  0.0032]])\n",
      "-->name: hidden1.bias -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([-0.0079,  0.0011, -0.0014, -0.0036])\n",
      "-->name: hidden2.weight -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([[ 7.9900e-04, -6.0455e-04, -2.4897e-05,  5.3192e-05],\n",
      "        [-6.2025e-03,  4.7794e-03,  1.9684e-04, -4.1012e-04],\n",
      "        [-2.1973e-04,  1.6667e-04,  6.8495e-06, -1.4609e-05],\n",
      "        [ 3.8799e-04, -2.9517e-04, -1.2190e-05,  2.5867e-05]])\n",
      "-->name: hidden2.bias -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([ 0.0011, -0.0085, -0.0003,  0.0005])\n",
      "-->name: predict1.weight -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([[0.0552, 0.0634, 0.0502, 0.0570, 0.0552, 0.0634, 0.0502, 0.0570]])\n",
      "-->name: predict1.bias -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([0.1041])\n",
      "-->name: predict2.weight -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " None\n",
      "-->name: predict2.bias -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " None\n",
      "Loss = 0.0953\n",
      "-->name: hidden1.weight -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([[ 0.0051,  0.0050,  0.0048],\n",
      "        [ 0.0007,  0.0008,  0.0008],\n",
      "        [-0.0014, -0.0015, -0.0015],\n",
      "        [ 0.0034,  0.0033,  0.0033]])\n",
      "-->name: hidden1.bias -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([-0.0072,  0.0010, -0.0017, -0.0037])\n",
      "-->name: hidden2.weight -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([[ 5.4795e-04, -5.1724e-04, -2.1333e-05,  3.7941e-05],\n",
      "        [-5.9459e-03,  5.7183e-03,  2.3587e-04, -4.0894e-04],\n",
      "        [-3.7557e-04,  3.5548e-04,  1.4630e-05, -2.5973e-05],\n",
      "        [ 1.6332e-04, -1.5496e-04, -6.4106e-06,  1.1324e-05]])\n",
      "-->name: hidden2.bias -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([ 3.0255e-04, -2.9493e-03, -2.0422e-04,  8.7329e-05])\n",
      "-->name: predict1.weight -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([[0.0239, 0.0256, 0.0193, 0.0195, 0.0239, 0.0256, 0.0193, 0.0195]])\n",
      "-->name: predict1.bias -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " tensor([0.0367])\n",
      "-->name: predict2.weight -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " None\n",
      "-->name: predict2.bias -->grad_requirs: True  \n",
      "-->grad_value:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "for t in range(2):\n",
    "#     prediction1, prediction2 = net(x1, x2)\n",
    "#     loss1 = loss_func(prediction1, y1)\n",
    "#     loss2 = loss_func(prediction2, y2)\n",
    "#     loss = loss1 + loss2 # 重点！\n",
    "    prediction1 = net(x1, x2)\n",
    "    loss = loss_func(prediction1, y1)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if t % 1 == 0:\n",
    "        print('Loss = %.4f' % loss.data)\n",
    "        for name, parms in net.named_parameters():\n",
    "            print('-->name:', name, '-->grad_requirs:',parms.requires_grad, ' \\n-->grad_value:\\n',parms.grad)\n",
    "    \n",
    "#     if t % 1 == 0:\n",
    "#         print('Loss1 = %.4f' % loss1.data,'Loss2 = %.4f' % loss2.data,)\n",
    "#         for name, parms in net.named_parameters():\n",
    "#             print('-->name:', name, '-->grad_requirs:',parms.requires_grad, ' \\n-->grad_value:\\n',parms.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLModel(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=16, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=14, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([6, 1, 2, 2])\n",
      "tensor([[-2.6457, -2.7180, -2.6952,  ..., -2.5762, -2.6698, -2.6415],\n",
      "        [-2.6242, -2.6836, -2.6679,  ..., -2.5679, -2.6864, -2.6482],\n",
      "        [-2.6299, -2.6830, -2.6731,  ..., -2.5654, -2.6906, -2.6618],\n",
      "        ...,\n",
      "        [-2.6286, -2.6972, -2.6643,  ..., -2.5779, -2.6780, -2.6510],\n",
      "        [-2.6307, -2.7015, -2.6931,  ..., -2.5712, -2.6843, -2.6566],\n",
      "        [-2.6327, -2.7043, -2.6622,  ..., -2.5577, -2.6874, -2.6625]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FLModel, self).__init__()\n",
    "        # 26.定义①的卷积层，输入为32x32的图像，卷积核大小5x5卷积核种类6\n",
    "        self.conv1 = nn.Conv2d(1, 6, 2)\n",
    "        # 27.定义③的卷积层，输入为前一层6个特征，卷积核大小5x5，卷积核种类16\n",
    "        self.conv2 = nn.Conv2d(6, 16, 2)\n",
    "        # 28.定义⑤的全链接层，输入为16*5*5，输出为120\n",
    "        self.fc1 = nn.Linear(16, 120)  # 6*6 from image dimension\n",
    "        # 29.定义⑥的全连接层，输入为120，输出为84\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 30.定义⑥的全连接层，输入为84，输出为10\n",
    "        self.fc3 = nn.Linear(84, 14)\n",
    "    def forward(self, x):\n",
    "        t3 = torch.zeros(256,2)\n",
    "        #print(x.size(),t3.size())\n",
    "        x = torch.cat((x, t3), 1)\n",
    "        x = x.view(256,1,9,9)\n",
    "        # 31.完成input-S2，先卷积+relu，再2x2下采样\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 32.完成S2-S4，先卷积+relu，再2x2下采样\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) #卷积核方形时，可以只写一个维度\n",
    "        # 33.将特征向量扁平成行向量\n",
    "        x = x.view(-1, 16)\n",
    "        # 34.使用fc1+relu\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 35.使用fc2+relu\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 36.使用fc3\n",
    "        x = self.fc3(x)\n",
    "#         return x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "net = FLModel()\n",
    "print(net)\n",
    "\n",
    "import numpy as np\n",
    "# 打印网络的参数\n",
    "params = list(net.parameters())\n",
    "# print(params)\n",
    "print(len(params))\n",
    "# 打印某一层参数的形状\n",
    "print(params[0].size())\n",
    "\n",
    "#随机输入一个向量，查看前向传播输出\n",
    "input = torch.randn(256,79)\n",
    "# input = np.expand_dims(input, axis=2)\n",
    "# print((input))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1486,  1.4084, -0.2635,  ..., -0.4677,  0.0000,  0.0000],\n",
       "        [-0.0549, -1.0925, -1.6236,  ...,  0.1509,  0.0000,  0.0000],\n",
       "        [-1.4069, -1.4782,  0.8286,  ...,  0.4287,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.2381, -0.6959, -0.6199,  ...,  2.3516,  0.0000,  0.0000],\n",
       "        [ 1.2532,  0.6781, -1.8472,  ..., -1.1791,  0.0000,  0.0000],\n",
       "        [-0.2224, -0.3319,  0.8432,  ...,  1.4974,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(64,79)\n",
    "t3 = torch.zeros(256,2)\n",
    "t4 = torch.zeros(64,2)\n",
    "t5 = torch.zeros(x.size()[0],2)\n",
    "x = torch.cat((x, t5), 1)\n",
    "#print(x.size(),t3.size())\n",
    "# try:\n",
    "#     x = torch.cat((x, t3), 1)\n",
    "# except:\n",
    "#     x = torch.cat((x, t4), 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5632/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*4*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([3, 4, 5], dtype=torch.int32)\n",
      "------------------------\n",
      "t2: tensor([0, 0, 0], dtype=torch.int32)\n",
      "------------------------\n",
      "t3: tensor([0., 0.])\n",
      "------------------------\n",
      "t4: tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array([3, 4, 5])\n",
    "b = np.array([0, 0, 0])\n",
    "\n",
    "t1 = torch.from_numpy(a)\n",
    "t2 = torch.from_numpy(b)\n",
    "t3 = torch.zeros(2)\n",
    "t4 = torch.ones(3)\n",
    "print('t1:', t1)\n",
    "print('------------------------')\n",
    "print('t2:', t2)\n",
    "print('------------------------')\n",
    "print('t3:', t3)\n",
    "print('------------------------')\n",
    "print('t4:', t4)\n",
    "\n",
    "res = torch.cat((t4, t3), 0)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.521739130434782"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "208/46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([170, 124,  46, 141,  18, 112, 173, 162, 151,  62,  24, 170, 121,\n",
       "        36, 105, 235,  98, 143, 163, 229, 163, 107, 137, 163, 141, 177,\n",
       "       179, 158, 123, 143, 133, 175, 126, 132, 218, 116, 149,  10,  14,\n",
       "        12, 126, 187, 149,  87, 119, 171, 217,  90, 106, 237,  79, 181,\n",
       "       177,  93, 165, 240, 205, 145,  86,  73,  35, 210, 222, 221,  60,\n",
       "       142, 135, 212, 215,  24,  43, 199, 129, 242, 246, 194,   6, 151,\n",
       "        75])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randint(0,255,[79])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53403124, -0.19091068, -1.42016002,  0.07700264, -1.86142902,\n",
       "       -0.38002596,  0.58131006,  0.40795438,  0.23459871, -1.16800631,\n",
       "       -1.76687138,  0.53403124, -0.2381895 , -1.57775609, -0.49034321,\n",
       "        1.55840569, -0.60066046,  0.10852185,  0.42371399,  1.46384805,\n",
       "        0.42371399, -0.458824  ,  0.01396421,  0.42371399,  0.07700264,\n",
       "        0.64434849,  0.6758677 ,  0.34491595, -0.20667029,  0.10852185,\n",
       "       -0.04907422,  0.61282927, -0.15939147, -0.06483383,  1.29049237,\n",
       "       -0.31698754,  0.20307949, -1.98750587, -1.92446744, -1.95598666,\n",
       "       -0.15939147,  0.80194456,  0.20307949, -0.77401614, -0.26970872,\n",
       "        0.54979084,  1.27473276, -0.72673732, -0.47458361,  1.5899249 ,\n",
       "       -0.90009299,  0.70738691,  0.64434849, -0.6794585 ,  0.4552332 ,\n",
       "        1.63720372,  1.08561748,  0.14004106, -0.78977575, -0.99465064,\n",
       "       -1.5935157 ,  1.16441552,  1.3535308 ,  1.33777119, -1.19952553,\n",
       "        0.09276224, -0.01755501,  1.19593473,  1.24321355, -1.76687138,\n",
       "       -1.46743884,  0.99105984, -0.11211265,  1.66872294,  1.73176137,\n",
       "        0.9122618 , -2.0505443 ,  0.23459871, -0.96313142])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "b = standardization(a)\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyngq",
   "language": "python",
   "name": "dyngq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
